{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020-2021 CertifAI Sdn. Bhd.\n",
    "#\n",
    "# This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b281522",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Feature selection is a **process where you select a number of features in your data that contribute most to the prediction** or remove the irrelevant and insignificant features. This helps to improve generalization, reduce overfitting, and even improve accuracy of model in some cases. It also saves the computational resources needed as you can train with smaller set of features.\n",
    "\n",
    "There are three types of feature selection: **Filter methods** (univariate statistics, Pearson correlation, variance thresholding), **Wrapper methods** (forward, backward, and exhaustive selection), and **Embedded methods** (Lasso, Ridge, Decision Tree). \n",
    "\n",
    "We will go into an explanation of each with examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78161d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# filter\n",
    "from sklearn.feature_selection import SelectKBest, chi2, VarianceThreshold, SelectFromModel\n",
    "\n",
    "# wrapper\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "\n",
    "# embedded\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5611ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "wine_data = load_wine()\n",
    "df = pd.DataFrame(data=wine_data.data,\n",
    "                  columns=wine_data.feature_names)\n",
    "\n",
    "# Adding the target variable\n",
    "df[\"target\"] = wine_data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8df4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output features\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d637f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d055175",
   "metadata": {},
   "source": [
    "We have 178 samples and 13 variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b70ea",
   "metadata": {},
   "source": [
    "## Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a2c7c",
   "metadata": {},
   "source": [
    "### Univariate Statistics\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. \n",
    "\n",
    "Scikit-learn univariate feature selection:\n",
    "\n",
    "- `SelectKBest` removes all but the  highest scoring features\n",
    "\n",
    "- `SelectPercentile` removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate `SelectFpr`, false discovery rate `SelectFdr`, or family wise error `SelectFwe`.\n",
    "\n",
    "- `GenericUnivariateSelect` allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.\n",
    "\n",
    "These objects take as input a scoring function that returns univariate scores and p-values (or only scores for `electKBest` and `SelectPercentile`):\n",
    "\n",
    "- For regression: `f_regression`, `mutual_info_regression`\n",
    "- For classification: `chi2`, `f_classif`, `mutual_info_classif`\n",
    "\n",
    "We will show an example of `SelectKBest` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69332086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature score:  [5.44549882e+00 2.80686046e+01 7.43380598e-01 2.93836955e+01\n",
      " 4.50263809e+01 1.56230759e+01 6.33343081e+01 1.81548480e+00\n",
      " 9.36828307e+00 1.09016647e+02 5.18253981e+00 2.33898834e+01\n",
      " 1.65400671e+04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 127.  ,    3.06,    5.64, 1065.  ],\n",
       "       [ 100.  ,    2.76,    4.38, 1050.  ],\n",
       "       [ 101.  ,    3.24,    5.68, 1185.  ],\n",
       "       [ 113.  ,    3.49,    7.8 , 1480.  ],\n",
       "       [ 118.  ,    2.69,    4.32,  735.  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 4 \"best\" features\n",
    "kbest = SelectKBest(score_func=chi2, k=4)\n",
    "kbest.fit(X, y)\n",
    "\n",
    "# Feature scores\n",
    "print(\"Feature score: \", kbest.scores_)\n",
    "\n",
    "new_features = kbest.transform(X)\n",
    "\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fe599",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient\n",
    "Correlation is a measure of the linear relationship of 2 or more variables. We would assume that the **good variables** are **highly correlated** with the target. Also, sometimes we would want to remove either one of the two variables that are highly correlated. \n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "  <img alt=\"Several sets of (x, y) points, with the correlation coefficient of x and y for each set.\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png\" width=\"400\" height=\"200\"><br>\n",
    "  <sup>Sample datasets and their pearson correlation coefficients.<sup>\n",
    "</div>\n",
    "      \n",
    "We will show an example that drop the variable which has a lower correlation coefficient value with the target variable. We need to set an absolute value, for example, 0.4 as the threshold for selecting the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1d4fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hue                             0.617369\n",
       "proline                         0.633717\n",
       "total_phenols                   0.719163\n",
       "od280/od315_of_diluted_wines    0.788230\n",
       "flavanoids                      0.847498\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pearson correlation coefficient\n",
    "corr = df.corr()[\"target\"].sort_values(ascending=False)[1:]\n",
    "\n",
    "# Absolute for positive values\n",
    "abs_corr = abs(corr)\n",
    "\n",
    "# Threshold for features to keep\n",
    "new_features = abs_corr[abs_corr > 0.6]\n",
    "new_features\n",
    "\n",
    "# new_df = df[new_features.index]\n",
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d0057",
   "metadata": {},
   "source": [
    "### Variance Threshold\n",
    "`VarianceThreshold` is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a85ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1.71,   15.6 ,  127.  ,    3.06,    5.64, 1065.  ],\n",
       "       [   1.78,   11.2 ,  100.  ,    2.76,    4.38, 1050.  ],\n",
       "       [   2.36,   18.6 ,  101.  ,    3.24,    5.68, 1185.  ],\n",
       "       [   1.95,   16.8 ,  113.  ,    3.49,    7.8 , 1480.  ],\n",
       "       [   2.59,   21.  ,  118.  ,    2.69,    4.32,  735.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining variance threshold\n",
    "vt = VarianceThreshold(threshold=0.7)\n",
    "new_features = vt.fit_transform(X)\n",
    "\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a94d9",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "\n",
    "*Scikit-learn* provides `SelectFromModel` which is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`).\n",
    "\n",
    "L1-based models and tree-based models can be used along with `SelectFromModel` to select the non-zero coefficients and discard irrelevant features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0afc2",
   "metadata": {},
   "source": [
    "1. L1-based feature selection\n",
    "- Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ccd3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  15.6 ,  127.  ,    3.06,    5.64, 1065.  ],\n",
       "       [  11.2 ,  100.  ,    2.76,    4.38, 1050.  ],\n",
       "       [  18.6 ,  101.  ,    3.24,    5.68, 1185.  ],\n",
       "       [  16.8 ,  113.  ,    3.49,    7.8 , 1480.  ],\n",
       "       [  21.  ,  118.  ,    2.69,    4.32,  735.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining L1-based model\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "\n",
    "# Select features from L1-based model\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "new_features = model.transform(X)\n",
    "\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6ea07",
   "metadata": {},
   "source": [
    "2. Tree-based feature selection\n",
    "- Tree-based estimators (see the *sklearn.tree* module and forest of trees in the *sklearn.ensemble* module) can be used to compute impurity-based feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72679892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:  [0.10609269 0.03175796 0.02467003 0.04995935 0.03373059 0.06044565\n",
      " 0.12563847 0.0200969  0.02791315 0.13603443 0.08649171 0.12532612\n",
      " 0.17184295]\n",
      "\n",
      "(178, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 3.060e+00, 5.640e+00, 1.040e+00, 3.920e+00, 1.065e+03],\n",
       "       [1.320e+01, 2.760e+00, 4.380e+00, 1.050e+00, 3.400e+00, 1.050e+03],\n",
       "       [1.316e+01, 3.240e+00, 5.680e+00, 1.030e+00, 3.170e+00, 1.185e+03],\n",
       "       [1.437e+01, 3.490e+00, 7.800e+00, 8.600e-01, 3.450e+00, 1.480e+03],\n",
       "       [1.324e+01, 2.690e+00, 4.320e+00, 1.040e+00, 2.930e+00, 7.350e+02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining tree-based model\n",
    "clf = ExtraTreesClassifier(n_estimators=50, random_state=1).fit(X, y)\n",
    "\n",
    "# Select features from tree-based model\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "new_features = model.transform(X)\n",
    "\n",
    "print(\"Feature importance: \", clf.feature_importances_, end=\"\\n\\n\")\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287a096",
   "metadata": {},
   "source": [
    "## Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2c078",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "\n",
    "First, the estimator is trained on the initial set of features and the importance of each feature is obtained. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13180d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 6\n",
      "Selected Features: [ True False  True False False  True  True False False False  True  True\n",
      " False]\n",
      "Feature Ranking: [1 5 1 3 8 1 1 2 6 4 1 1 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.23,  2.43,  2.8 ,  3.06,  1.04,  3.92],\n",
       "       [13.2 ,  2.14,  2.65,  2.76,  1.05,  3.4 ],\n",
       "       [13.16,  2.67,  2.8 ,  3.24,  1.03,  3.17],\n",
       "       [14.37,  2.5 ,  3.85,  3.49,  0.86,  3.45],\n",
       "       [13.24,  2.87,  2.8 ,  2.69,  1.04,  2.93]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining model to build\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Create the RFE model and select 6 attributes\n",
    "rfe = RFE(lin_reg, n_features_to_select=6)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Summarize the selection of the attributes\n",
    "print(\"Num Features: %s\" % (rfe.n_features_))\n",
    "print(\"Selected Features: %s\" % (rfe.support_))\n",
    "print(\"Feature Ranking: %s\" % (rfe.ranking_))\n",
    "\n",
    "new_features = rfe.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a6717",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "The procedure starts with an empty set of features. The best of the original features is determined and added to the reduced set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "453322b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15.6 ,    3.06,    5.64, 1065.  ],\n",
       "       [  11.2 ,    2.76,    4.38, 1050.  ],\n",
       "       [  18.6 ,    3.24,    5.68, 1185.  ],\n",
       "       [  16.8 ,    3.49,    7.8 , 1480.  ],\n",
       "       [  21.  ,    2.69,    4.32,  735.  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the forward feature selector\n",
    "ffs = SequentialFeatureSelector(\n",
    "    lin_reg, n_features_to_select=4, direction=\"forward\")\n",
    "ffs.fit(X, y)\n",
    "\n",
    "new_features = ffs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad3c9c",
   "metadata": {},
   "source": [
    "### Backward Feature Elimination\n",
    " The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e7ad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15.6 ,    3.06,    5.64, 1065.  ],\n",
       "       [  11.2 ,    2.76,    4.38, 1050.  ],\n",
       "       [  18.6 ,    3.24,    5.68, 1185.  ],\n",
       "       [  16.8 ,    3.49,    7.8 , 1480.  ],\n",
       "       [  21.  ,    2.69,    4.32,  735.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the backward feature selector\n",
    "bfs = SequentialFeatureSelector(\n",
    "    lin_reg, n_features_to_select=4, direction=\"backward\", cv=2)\n",
    "bfs.fit(X, y)\n",
    "\n",
    "new_features = bfs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c5714",
   "metadata": {},
   "source": [
    "### Exhaustive Feature Selection\n",
    "This is a brute-force evaluation of each feature subset. It tries every possible combination of the variables and returns the best performing subset but also take longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8b3840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 1079/1079"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.23,  2.8 ,  3.06,  5.64],\n",
       "       [13.2 ,  2.65,  2.76,  4.38],\n",
       "       [13.16,  2.8 ,  3.24,  5.68],\n",
       "       [14.37,  3.85,  3.49,  7.8 ],\n",
       "       [13.24,  2.8 ,  2.69,  4.32]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Create the exhaustive feature selector\n",
    "efs = ExhaustiveFeatureSelector(knn, min_features=2, max_features=4)\n",
    "efs.fit(X, y)\n",
    "\n",
    "new_features = efs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb53c95",
   "metadata": {},
   "source": [
    "## Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae265a",
   "metadata": {},
   "source": [
    "### LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "This type of regularization (L1) can lead to zero coefficients. Lasso selects the only some feature while reduces the coefficients of others to zero. \n",
    "<div align=\"center\">\n",
    "  <img alt=\"\" src=\"https://user-images.githubusercontent.com/79887667/134531133-7bd90082-ace5-47f9-a4b4-20ab6fca1505.png\" width=\"400\" height=\"200\"><br>\n",
    "  <sup>Cost function for Lasso regression<sup>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac111a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alcalinity_of_ash', 'color_intensity', 'proline']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Lasso model\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Perform feature selection\n",
    "new_features = [feature for feature, weight in zip(\n",
    "    X.columns.values, lasso.coef_) if weight != 0]\n",
    "\n",
    "print(len(new_features))\n",
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a7475",
   "metadata": {},
   "source": [
    "In order to better understand the effect of regularization, here is a helper function that will  print out the function fit by the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54db45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper method for pretty-printing the coefficients\n",
    "def pretty_print_coefs(coefs, names=None, sort=False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key=lambda x: -np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                      for coef, name in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f19eb411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model: 0.0 * X0 + 0.0 * X1 + 0.0 * X2 + 0.004 * X3 + 0.0 * X4 + -0.0 * X5 + -0.0 * X6 + 0.0 * X7 + -0.0 * X8 + 0.068 * X9 + -0.0 * X10 + -0.0 * X11 + -0.002 * X12\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso model:\", pretty_print_coefs(lasso.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b93538",
   "metadata": {},
   "source": [
    "### Tree-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d0b4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12173001 0.04046114 0.0191245  0.03167711 0.02425761 0.0941524\n",
      " 0.0882276  0.0408133  0.0304245  0.11278127 0.09579309 0.10365887\n",
      " 0.1968986 ]\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alcohol', 'color_intensity', 'od280/od315_of_diluted_wines', 'proline']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train extra trees classifier\n",
    "model = ExtraTreesClassifier(n_estimators=10, random_state=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "\n",
    "# Perform feature selection\n",
    "new_features = [feature for feature, weight in zip(\n",
    "    X.columns.values, model.feature_importances_) if weight > 0.1]\n",
    "\n",
    "print(len(new_features))\n",
    "new_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
