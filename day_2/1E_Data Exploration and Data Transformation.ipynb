{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![logo](https://user-images.githubusercontent.com/59526258/124226124-27125b80-db3b-11eb-8ba1-488d88018ebb.png)\r\n",
    "\r\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\r\n",
    " <br>\r\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\r\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\r\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\r\n",
    "<br>(at your option) any later version.\r\n",
    "<br>\r\n",
    "<br>This program is distributed in the hope that it will be useful,\r\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\n",
    "<br>GNU Affero General Public License for more details.\r\n",
    "<br>\r\n",
    "<br>You should have received a copy of the GNU Affero General Public License\r\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\r\n",
    "<br>"
   ],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore Statistics by Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn.preprocessing import PowerTransformer\r\n",
    "from scipy.stats import skewnorm\r\n",
    "from sklearn.preprocessing import FunctionTransformer\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a small dataset that contains (Physics,Biology and Maths) marks of a classroom of students."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the comma-seperated file(csv) that contains the marks. We assign the \"Names\" column as our index"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"https://archive.org/download/ml-fundamentals-data/machine-learning-fundamentals-data/grades.csv\",index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the first 5 rows of data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show all the data entries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the first 5 rows of ```biology``` data column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"\"].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Describe the dataset.\n",
    "- count: Total number of valid data. Will ignore null\n",
    "- mean: Mean value of data\n",
    "- std: Standard deviation of data\n",
    "- min: Minimum value of data\n",
    "- 25%, 50%, 75%: Percentiles of data. This can be specifed from the parameters by passing ```percentiles``` as list format. Default value is [.25, .5, .75]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the information about you data frame, e.g. Columns, Data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show available columns of data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df..values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot a **bar chart** of the grades data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.(kind=\"bar\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot a **box plot** of the grades data.\n",
    "\n",
    "### Boxplot parameters\n",
    "<img src=\"https://matplotlib.org/3.2.2/_images/boxplot_explanation.png\" width=\"500\"/>\n",
    "\n",
    "[Image Source: Matplotlib](https://matplotlib.org/3.2.2/faq/howto_faq.html)\n",
    "\n",
    "### Understanding boxplot\n",
    "- Boxplot is a method to display the distribution of data\n",
    "- The Interquartile Range(IQR) indicates the range where most data is spread. We can use this to observe the spread of data. In other words, the data is concentrated in the IRQ. \n",
    "- Simple interpretation:\n",
    "    - For Biology, we can understand that most student score in between 59 to 79. \n",
    "    - Also, someone score below the expected minimum. Note that the expected minimum is only a boxplot indicator. It is not the minimum score from the data. The expected minimum value is calculated using the formula above. From this, we can understand that someone underachieved the test. This is called lower outlier. If someone score over the expected maximum, it is called higher outlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show boxplot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram parameters\n",
    "- x-axis: observed value\n",
    "- y-axis: frequency of occurences\n",
    "\n",
    "### Understanding histogram\n",
    "- Histogram can be used to evaluate frequency of value.\n",
    "\n",
    "Plot the **histograms** of the grades data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot only the histogram of \"Physics\" column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"Physics\"].()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot a distribution plot by using **seaborn** module."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.(df[\"Physics\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check how \"normally distributed\" a distribution is by checking the skewness of the distribution.\n",
    "- A skewness value of 0 indicates a symmetrical distribution of values.\n",
    "- A negative skewness value indicates an asymmetry in the distribution and the tail is larger towards the left hand side of the distribution(Left skewed).\n",
    "- A positive skewness value indicates an asymmetry in the distribution and the tail is larger towards the right hand side of the distribution(Right skewed)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the skewness of all columns or only 1 column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"Physics\"].()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transformation 1 - Skewness"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In many Machine Learning modeling scenarios, **normality** of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a **Gaussian distribution** as possible in order to stabilize variance and **minimize skewness**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer = PowerTransformer(method='box-cox', standardize=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[\"Physics\"].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently, the data looks like this:\n",
    "```example_data = [1, 2, 3, 4, 5, 6]```\n",
    "The example_data has 6 elements arranged as single 1d array (also known as vector). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View df[\"Physics\"] in list\n",
    "\n",
    "print(df[\"Physics\"].tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Transformer accepts 2d array, which is not compatible to our 1D data. We will first transform the shape of data with reshape()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_2d = df[\"Physics\"].values.reshape(-1,1)\n",
    "data_2d.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every data in the 1D array is transformed into their own list of size (1, 1) and combined together with other data to produce (26, 1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View transformed data in list\n",
    "\n",
    "print(data_2d.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the data and check the values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_trans = transformer.fit_transform(data_2d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new = pd.DataFrame(data_trans,index=df.index)\n",
    "df_new.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename the column to ```Physics``` according to the original. This is not a required step. But, if you have more than one column, renaming is a good practice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new.rename(columns={0: \"Physics\"}, inplace=True)\n",
    "df_new.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the histogram of the transformed \"Physics\" marks and visualize the distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new.hist()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "splot = sns.distplot(df_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plotted distribution graph looks a like a **normal distribution**, but due to the dataset, the transformation is not very obvious.\n",
    "\n",
    "We will compare the **skewness** to assert that the data transformation has made the distribution more \"normally distributed\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Skewness before: {}\".format(df[\"Physics\"].skew()))\n",
    "print(\"Skewness after: {}\".format(df_new.skew().squeeze())) # squeeze() convert 1D object to scalar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us try with our own generated distribution. We will generate a distribution that is **greatly skewed**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# skewnorm will generate random numbers\n",
    "\n",
    "rand_vars = skewnorm.rvs(5, size=10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(rand_vars)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(rand_vars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.distplot(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, we have 10000 random variables are the distribution is non-normal distribution and greatly skewed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us check the skewness of our generated data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.skew().squeeze()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our dataset is positive skewed. This round, we will use **Log Transform** to transform our data and observe the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer = FunctionTransformer(np.log1p, validate=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_trans = transformer.transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new = pd.DataFrame(data_trans)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(data_trans)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Skewness before: {}\".format(df.skew().squeeze()))\n",
    "print(\"Skewness after: {}\".format(df_new.skew().squeeze()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can the skewness is greatly reduced and the distribution resembles a **normal distribution**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.distplot(df_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transformation 2 - One-hot encoding\n",
    "\n",
    "Another frequent encounter of data is the value are not in number. The example below shows a string dataset. Sklearn algorithm cannot process string data. So, the data need to be represented as numbers. One method is to perform one-hot encoding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = [\"Jack\", \"Jill\", \"Mary\", \"Jack\", \"Jill\", \"Jill\"]\n",
    "df = pd.DataFrame(x, columns=[\"Name\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "get_dummies will create new columns according to all variables in the specified column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_one_hot = pd.get_dummies(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As seen above, all variables are converted to columns and the column that is associated to it is marked as 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transformation 3 - Ordinal Variables\n",
    "\n",
    "This technique transform the data by assigning labels according to a ranges or data group. An ordinal variable is similar to categorical variable except there is a sense of order to the labelled data.\n",
    "\n",
    "For example, in clothing, rather than dealing with continuous chest size, shirt length, etc. for measurement, consumer use labels such as small, medium, large, etc. to define the size. The order is from small to large.\n",
    "\n",
    "The example below shows an application for households income range in Malaysia. The defined labels are based on 2019 income thresholds. [Source](https://ringgitplus.com/en/blog/personal-finance-news/dosm-survey-higher-income-thresholds-for-b40-m40-t20-households-in-2019.html#:~:text=According%20to%20the%20report%2C%20the,960%20and%20above%20for%20T20.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate 10 random numbers range 1000 to 20000 indicating households income\n",
    "min_limit = 1000\n",
    "max_limit = 20000\n",
    "a = np.random.randint(low=min_limit, high=max_limit, size=10) \n",
    "df = pd.DataFrame(a, columns=[\"data\"])\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bins = [min_limit, 4850, 10959, max_limit]\n",
    "labels = [\"B40\", \"M40\", \"T20\"]\n",
    "\n",
    "df[\"encoded\"] = pd.cut(df[\"data\"], bins=bins, labels=labels, include_lowest=True)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all the data are encoded according to the range of bins. Then you can one-hot encode them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_one_hot = pd.get_dummies(df[\"encoded\"])\n",
    "df = pd.concat([df[\"data\"], df_one_hot], axis=1)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the dataframe are transformed into their respective labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Transformation 4 - Handle Missing data\n",
    "In the real-world dataset, there will be missing data which most of the time, represented as NaN.\n",
    "\n",
    "Not all missing data can be handled. One example is string data such as address or product name. There is no way to approximate the data by itself. \n",
    "\n",
    "Adding artificial data may or may not work well as it will introduce noise, especially if the missing data is the label. Thus, one handling method is to remove the row. Handle missing data is more of experimentation to see which works the best.\n",
    "\n",
    "In this example, we are going to look at the sklearn function called SimpleImputer.\n",
    "\n",
    "Consider the data below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate random data with NaN\n",
    "a = np.random.randn(6)\n",
    "idx = np.random.randint(len(a)) \n",
    "a[idx] = np.nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(a, columns=[\"My Data\"])\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "df_fixed = imp.fit_transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_fixed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### String data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = [\"jack\", \"jill\", \"john\", \"ali\", np.nan, \"jack\"]\n",
    "df = pd.DataFrame(a, columns=[\"name\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imp = SimpleImputer(strategy=\"constant\", fill_value=\"abu\")\n",
    "df_fixed = imp.fit_transform(df)\n",
    "print(df_fixed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SimpleImputer provides four strategies to impute missing data\n",
    "- mean: replace with mean along each column\n",
    "- median: replace with median along each column\n",
    "- most_frequent: replace with the most frequent value along each column\n",
    "- constant: replace with a specified fill_value. Can be used for strings\n",
    "\n",
    "It is important to understand that handling missing data is just an approximation. As you can see with SimpleImputer, the strategies are based on statistical technique(except constant), which, depends on overall column values. Imputing value using \"noisy\" data will add more noise to the data."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}