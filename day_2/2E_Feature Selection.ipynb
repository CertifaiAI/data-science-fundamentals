{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020d40f6",
   "metadata": {},
   "source": [
    "![logo](https://user-images.githubusercontent.com/59526258/124226124-27125b80-db3b-11eb-8ba1-488d88018ebb.png)\n",
    "\n",
    "> **Copyright (c) 2021 CertifAI Sdn. Bhd.**<br>\n",
    " <br>\n",
    "This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "<br>it under the terms of the GNU Affero General Public License as published by\n",
    "<br>the Free Software Foundation, either version 3 of the License, or\n",
    "<br>(at your option) any later version.\n",
    "<br>\n",
    "<br>This program is distributed in the hope that it will be useful,\n",
    "<br>but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "<br>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "<br>GNU Affero General Public License for more details.\n",
    "<br>\n",
    "<br>You should have received a copy of the GNU Affero General Public License\n",
    "<br>along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340150a",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "1. [Filter Methods](#filter)\n",
    "    1. Univariate Statistics\n",
    "    2. Correlation Coefficient\n",
    "    3. Variance Threshold \n",
    "    <br/><br/>\n",
    "2. [Wrapper Methods](#wrapper)\n",
    "    1. Recursive Feature Elimination (RFE)\n",
    "    2. Forward Feature Selection\n",
    "    3. Backward Feature Elimination\n",
    "    4. Exhaustive Feature Selection\n",
    "    <br/><br/>\n",
    "3. [Embedded Methods](#embedded)\n",
    "    1. LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "        - `LinearSVC` with `SelectFromModel`\n",
    "        - `Lasso` with Thresholding\n",
    "    2. Tree-based Model\n",
    "        - `ExtraTreesClassifier` with `SelectFromModel`\n",
    "        - `ExtraTreesClassifier` with Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18346c",
   "metadata": {},
   "source": [
    "## Learning Outcome\n",
    "After going through this notebook, you should be able to: \n",
    "1. Perform filter methods to select features\n",
    "2. Perform wrapper methods to select features\n",
    "3. Perform embedded methods to select features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33a7ee",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Feature selection is a **process where you select a number of features in your data that contribute most to the prediction** or remove the irrelevant and insignificant features. This helps to improve generalization, reduce overfitting, and even improve accuracy of model in some cases. It also saves the computational resources needed as you can train with smaller set of features.\n",
    "\n",
    "There are three types of feature selection: **Filter methods** (univariate statistics, Pearson correlation, variance thresholding), **Wrapper methods** (forward, backward, and exhaustive selection), and **Embedded methods** (Lasso, Ridge, Decision Tree). \n",
    "\n",
    "We will go into an explanation of each with examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# filter\n",
    "from sklearn.feature_selection import SelectKBest, chi2, VarianceThreshold, SelectFromModel\n",
    "\n",
    "# wrapper\n",
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "\n",
    "# embedded\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "wine_data = load_wine()\n",
    "df = pd.DataFrame(data=wine_data.data,\n",
    "                  columns=wine_data.feature_names)\n",
    "\n",
    "# Adding the target variable\n",
    "df[\"target\"] = wine_data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output features\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddd6ef",
   "metadata": {},
   "source": [
    "We have 178 samples and 13 variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5058d",
   "metadata": {},
   "source": [
    "## Filter Methods <a id=\"filter\"></a>\n",
    "Features are filtered based on general characteristics (some metric such as correlation) of the dataset. Filter method is performed without any predictive model.\n",
    "\n",
    "It is faster and usually the better approach when the number of features are huge. Avoids overfitting but sometimes may fail to select best features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fedbe2",
   "metadata": {},
   "source": [
    "### Univariate Statistics\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. \n",
    "\n",
    "Scikit-learn univariate feature selection:\n",
    "\n",
    "- `SelectKBest` removes all but the  highest scoring features\n",
    "\n",
    "- `SelectPercentile` removes all but a user-specified highest scoring percentage of features \n",
    "\n",
    "- `GenericUnivariateSelect` allows to perform univariate feature selection with a configurable strategy\n",
    "\n",
    "These objects take as input a scoring function that returns univariate scores and p-values (or only scores for `SelectKBest` and `SelectPercentile`):\n",
    "\n",
    "- For regression: `f_regression`, `mutual_info_regression`\n",
    "- For classification: `chi2`, `f_classif`, `mutual_info_classif`\n",
    "\n",
    "We will show an example of `SelectKBest` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Use SlectKBest to select 4 features\n",
    "kbest = \n",
    "kbest.\n",
    "\n",
    "# Feature scores\n",
    "print(\"Feature score: \", kbest.scores_)\n",
    "\n",
    "new_features = kbest.transform(X)\n",
    "\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76a9cc",
   "metadata": {},
   "source": [
    "### Correlation Coefficient\n",
    "Correlation is a measure of the linear relationship of 2 or more variables. We would assume that the **good variables** are **highly correlated** with the target. Also, sometimes we would want to remove either one of the two variables that are highly correlated. \n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "  <img alt=\"Several sets of (x, y) points, with the correlation coefficient of x and y for each set.\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1920px-Correlation_examples2.svg.png\" width=\"400\" height=\"200\"><br>\n",
    "  <sup>Sample datasets and their pearson correlation coefficients.<sup>\n",
    "</div>\n",
    "      \n",
    "We will show an example that drop the variable which has a lower *Pearson* correlation coefficient value with the target variable. We need to set an absolute value, for example, 0.4 as the threshold for selecting the variables.\n",
    "      \n",
    "You may use other correlation coefficient such as *Spearman* or *Kendall*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation coefficient\n",
    "corr = df.corr()[\"target\"].sort_values(ascending=False)[1:]\n",
    "\n",
    "# Absolute for positive values\n",
    "abs_corr = abs(corr)\n",
    "\n",
    "#TODO : Threshold for features to keep\n",
    "new_features = \n",
    "new_features\n",
    "\n",
    "# new_df = df[new_features.index]\n",
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d27e1",
   "metadata": {},
   "source": [
    "### Variance Threshold\n",
    "`VarianceThreshold` is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "The estimator only works with numeric data and it will raise an error if there are categorical features present in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53eb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : VarianceThreshold to keep features with variance higher than 0.7 \n",
    "vt = \n",
    "new_features = vt.\n",
    "\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3bf566",
   "metadata": {},
   "source": [
    "## Wrapper Methods <a id=\"wrapper\"></a>\n",
    "The feature selection algorithm is like a wrapper around the predictive model algorithm and uses the same model to select best features. \n",
    "\n",
    "It tends to give better performance as it search through a wider variety of predictor subsets. Though computationally expensive and prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d6943",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "Recursive feature elimination (RFE) selects features by recursively considering smaller and smaller sets of features. \n",
    "\n",
    "First, the estimator is trained on the initial set of features and the importance of each feature is obtained. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85472cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model to build\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "#TODO : Create a RFE and select 6 features\n",
    "rfe = \n",
    "rfe.\n",
    "\n",
    "# Summarize the selection of the attributes\n",
    "print(\"Num Features: %s\" % (rfe.n_features_))\n",
    "print(\"Selected Features: %s\" % (rfe.support_))\n",
    "print(\"Feature Ranking: %s\" % (rfe.ranking_))\n",
    "\n",
    "new_features = rfe.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4de525",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "The procedure starts with an empty set of features. The best of the original features is determined and added to the reduced set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78346cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Create a forward feature selector and select 4 features\n",
    "ffs = \n",
    "ffs.\n",
    "\n",
    "new_features = ffs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810e6b6",
   "metadata": {},
   "source": [
    "### Backward Feature Elimination\n",
    " The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed44650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Create a backward feature selector and select 4 features\n",
    "bfs = \n",
    "bfs.\n",
    "\n",
    "new_features = bfs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1c8b0",
   "metadata": {},
   "source": [
    "### Exhaustive Feature Selection\n",
    "This is a brute-force evaluation of each feature subset. It tries every possible combination of the variables and returns the best performing subset but also takes longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff403bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#TODO : Create an exhaustive feature selector and select 4 features\n",
    "efs = \n",
    "efs.\n",
    "\n",
    "new_features = efs.transform(X)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24703ca",
   "metadata": {},
   "source": [
    "## Embedded Methods <a id=\"embedded\"></a>\n",
    "Feature selection process is embedded in the learning or the model building phase. \n",
    "\n",
    "It is less computationally expensive than wrapper method and less prone to overfitting. However, it is model specific (only certain models have intrinsic feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea78b98",
   "metadata": {},
   "source": [
    "**SelectFromModel**\n",
    "\n",
    "*Scikit-learn* provides `SelectFromModel` which is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`).\n",
    "\n",
    "**L1-based models** and **tree-based models** can be used along with `SelectFromModel` to select the non-zero coefficients and discard irrelevant features. Otherwise, we can select features with simple thresholding.\n",
    "\n",
    "In the example below, we will show two feature selection methods for both L1-based models and tree-based models:\n",
    "1. Model with `SelectFromModel`\n",
    "2. Model with Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503edad",
   "metadata": {},
   "source": [
    "### LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "This type of regularization (L1) can lead to zero coefficients. Lasso selects the only some feature while reduces the coefficients of others to zero. \n",
    "<div align=\"center\">\n",
    "  <img alt=\"\" src=\"https://user-images.githubusercontent.com/79887667/134531133-7bd90082-ace5-47f9-a4b4-20ab6fca1505.png\" width=\"400\" height=\"200\"><br>\n",
    "  <sup>Cost function for Lasso regression<sup>\n",
    "</div>\n",
    "Linear models penalized with the L1 norm will have sparse solutions: many of their estimated coefficients are zero. Thus, they are feature selectors by nature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e9b90",
   "metadata": {},
   "source": [
    "We can use a linear model with `penalty=\"l1\"` or the `Lasso` module from *scikit-learn*, which both are linear model trained with L1 prior as regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfe32f",
   "metadata": {},
   "source": [
    "1. `LinearSVC` with `SelectFromModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67569f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define a LinearSVC model with \"l1\" penalty\n",
    "lsvc = \n",
    "\n",
    "#TODO : Select features from L1-based model with SelectFromModel\n",
    "model = \n",
    "new_features = model.\n",
    "\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb75006",
   "metadata": {},
   "source": [
    "2. `Lasso` with Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00963ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Train a Lasso model which includes feature selection internally\n",
    "lasso = \n",
    "lasso.\n",
    "\n",
    "# Perform feature selection\n",
    "new_features = [feature for feature, weight in zip(\n",
    "    X.columns.values, lasso.coef_) if weight != 0]\n",
    "\n",
    "print(len(new_features))\n",
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed29c1",
   "metadata": {},
   "source": [
    "In order to better understand the effect of regularization, here is a helper function that will  print out the function fit by the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f21051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper method for pretty-printing the coefficients\n",
    "def pretty_print_coefs(coefs, names=None, sort=False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key=lambda x: -np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                      for coef, name in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso model:\", pretty_print_coefs(lasso.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0db599",
   "metadata": {},
   "source": [
    "### Tree-based Model\n",
    "Tree-based estimators (see the `sklearn.tree` module and forest of trees in the `sklearn.ensemble` module) can be used to compute impurity-based feature importances.\n",
    "\n",
    "Feature importances are provided by the fitted attribute `feature_importances_`. Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\n",
    "\n",
    "When training a tree, the more a feature decreases the impurity, the more important the feature is. Generally, features that are selected at the top of the trees are more important than features that are selected at the end nodes of the trees, as the top splits lead to bigger information gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388dd0f",
   "metadata": {},
   "source": [
    "1. `ExtraTreesClassifier` with `SelectFromModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4592c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define a ExtraTreesClassifier model\n",
    "clf = ExtraTreesClassifier(n_estimators=50, random_state=1).fit(X, y)\n",
    "\n",
    "#TODO : Select features from tree-based model with SelectFromModel\n",
    "model = \n",
    "new_features = model.\n",
    "\n",
    "print(\"Feature importance: \", clf.feature_importances_, end=\"\\n\\n\")\n",
    "print(new_features.shape)\n",
    "new_features[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14fe89",
   "metadata": {},
   "source": [
    "2. `ExtraTreesClassifier` with Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aadce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Train an ExtraTreesClassifier which calculate the feature importances internally\n",
    "model = ExtraTreesClassifier(n_estimators=10, random_state=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "\n",
    "# Perform feature selection\n",
    "new_features = [feature for feature, weight in zip(\n",
    "    X.columns.values, model.feature_importances_) if weight > 0.1]\n",
    "\n",
    "print(len(new_features))\n",
    "new_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdle",
   "language": "python",
   "name": "cdle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
