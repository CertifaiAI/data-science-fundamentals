{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2020-2021 CertifAI Sdn. Bhd.\n",
    "# \n",
    "# This program is part of OSRFramework. You can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "# \n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "# \n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Classification by using K Nearest Neighbour (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Iris Dataset With KNN\n",
    "### Load Data\n",
    "Here we will load the IRIS dataset from scikit-learn. We will be utilizing `iris.data` and `iris.target` as usual for our features and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual `dir(iris)` shows the attributes of the iris datasets.<br> `iris.data.shape` shows the shape of the data.<br>\n",
    "`iris.target_names` shows the classes that we want to classify.<br>\n",
    "`iris.feature_names` shows the name of features that we are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris.data.astype(np.float32)\n",
    "target = iris.target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    data, target, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (105,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45, 4), (45,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We will use K Nearest Neighbours from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model.<br>\n",
    "Specify the number of neighbors to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by using train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Enter the code to call fit the training data into the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `metrics.confusion_matrix` will visualize the performance of the model through a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  1 16]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Glass Dataset from UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Here, we load the glass data from UCI ML Repository into a Dataframe using **pandas**.<br> `glass` will be storing the dataset, `description` will store the text with the description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\", \n",
    "    names=['ID','Refractive Index','Na','Mg','Al','Si','K','Ca','Ba','Fe','Class']\n",
    ")\n",
    "description = req.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.names\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Glass Identification Database\n",
      "\n",
      "2. Sources:\n",
      "    (a) Creator: B. German\n",
      "        -- Central Research Establishment\n",
      "           Home Office Forensic Science Service\n",
      "           Aldermaston, Reading, Berkshire RG7 4PN\n",
      "    (b) Donor: Vina Spiehler, Ph.D., DABFT\n",
      "               Diagnostic Products Corporation\n",
      "               (213) 776-0180 (ext 3014)\n",
      "    (c) Date: September, 1987\n",
      "\n",
      "3. Past Usage:\n",
      "    -- Rule Induction in Forensic Science\n",
      "       -- Ian W. Evett and Ernest J. Spiehler\n",
      "       -- Central Research Establishment\n",
      "          Home Office Forensic Science Service\n",
      "          Aldermaston, Reading, Berkshire RG7 4PN\n",
      "       -- Unknown technical note number (sorry, not listed here)\n",
      "       -- General Results: nearest neighbor held its own with respect to the\n",
      "             rule-based system\n",
      "\n",
      "4. Relevant Information:n\n",
      "      Vina conducted a comparison test of her rule-based system, BEAGLE, the\n",
      "      nearest-neighbor algorithm, and discriminant analysis.  BEAGLE is \n",
      "      a product available through VRS Consulting, Inc.; 4676 Admiralty Way,\n",
      "      Suite 206; Marina Del Ray, CA 90292 (213) 827-7890 and FAX: -3189.\n",
      "      In determining whether the glass was a type of \"float\" glass or not,\n",
      "      the following results were obtained (# incorrect answers):\n",
      "\n",
      "             Type of Sample                            Beagle   NN    DA\n",
      "             Windows that were float processed (87)     10      12    21\n",
      "             Windows that were not:            (76)     19      16    22\n",
      "\n",
      "      The study of classification of types of glass was motivated by \n",
      "      criminological investigation.  At the scene of the crime, the glass left\n",
      "      can be used as evidence...if it is correctly identified!\n",
      "\n",
      "5. Number of Instances: 214\n",
      "\n",
      "6. Number of Attributes: 10 (including an Id#) plus the class attribute\n",
      "   -- all attributes are continuously valued\n",
      "\n",
      "7. Attribute Information:\n",
      "   1. Id number: 1 to 214\n",
      "   2. RI: refractive index\n",
      "   3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as \n",
      "                  are attributes 4-10)\n",
      "   4. Mg: Magnesium\n",
      "   5. Al: Aluminum\n",
      "   6. Si: Silicon\n",
      "   7. K: Potassium\n",
      "   8. Ca: Calcium\n",
      "   9. Ba: Barium\n",
      "  10. Fe: Iron\n",
      "  11. Type of glass: (class attribute)\n",
      "      -- 1 building_windows_float_processed\n",
      "      -- 2 building_windows_non_float_processed\n",
      "      -- 3 vehicle_windows_float_processed\n",
      "      -- 4 vehicle_windows_non_float_processed (none in this database)\n",
      "      -- 5 containers\n",
      "      -- 6 tableware\n",
      "      -- 7 headlamps\n",
      "\n",
      "8. Missing Attribute Values: None\n",
      "\n",
      "Summary Statistics:\n",
      "Attribute:   Min     Max      Mean     SD      Correlation with class\n",
      " 2. RI:       1.5112  1.5339   1.5184  0.0030  -0.1642\n",
      " 3. Na:      10.73   17.38    13.4079  0.8166   0.5030\n",
      " 4. Mg:       0       4.49     2.6845  1.4424  -0.7447\n",
      " 5. Al:       0.29    3.5      1.4449  0.4993   0.5988\n",
      " 6. Si:      69.81   75.41    72.6509  0.7745   0.1515\n",
      " 7. K:        0       6.21     0.4971  0.6522  -0.0100\n",
      " 8. Ca:       5.43   16.19     8.9570  1.4232   0.0007\n",
      " 9. Ba:       0       3.15     0.1750  0.4972   0.5751\n",
      "10. Fe:       0       0.51     0.0570  0.0974  -0.1879\n",
      "\n",
      "9. Class Distribution: (out of 214 total instances)\n",
      "    -- 163 Window glass (building windows and vehicle windows)\n",
      "       -- 87 float processed  \n",
      "          -- 70 building windows\n",
      "          -- 17 vehicle windows\n",
      "       -- 76 non-float processed\n",
      "          -- 76 building windows\n",
      "          -- 0 vehicle windows\n",
      "    -- 51 Non-window glass\n",
      "       -- 13 containers\n",
      "       -- 9 tableware\n",
      "       -- 29 headlamps\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `glass` dataset is a combination of features and categories. From the description, we know that the features that we are interested are in columns **2 - 10**. <br>It is common practice that most of the data have their **expected value/ categories** in the last column, which is also the case in this dataset.<br><br> Using `iloc`, separate the data into `glass_data` which contains features, and `glass_target` which contains expected values/ categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_data = glass.iloc[:,1:-1]\n",
    "glass_target = glass.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the amounts of data in each class varies too much. This is a showcase of what's called **imbalanced data**.<br><br>\n",
    "There are a few ways to tackle this problem. Here, we are choosing to use a method called **oversampling**.<br><br>\n",
    "**Oversampling** refers to increasing the number of data points in the minority classes.<br><br>\n",
    "There are a few techniques for oversampling:\n",
    "1. Random sampling\n",
    "2. SMOTE: Synthetic Minority Over-sampling Technique\n",
    "3. ADASYN: Adaptive Synthetic Sampling\n",
    "\n",
    "For more details about oversampling do refer to https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/.<br><br>\n",
    "In this case, we are going to utilize `SMOTE` as `SMOTE` can avoid overfitting.To oversample the data, we are going to utilize a external library called `imblearn`.<br><i>Note: To install this library, run this command: `pip install imblearn` in command line/ terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (from imblearn) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (from imbalanced-learn->imblearn) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (from imbalanced-learn->imblearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\guanyu.tan\\anaconda\\envs\\test\\lib\\site-packages (from imbalanced-learn->imblearn) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "glass_data, glass_target = oversample.fit_resample(glass_data,glass_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `glass_data` into **test and train data**.<br>Test size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(\n",
    "    glass_data, glass_target, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **feature scaling** on the `X_train2`,`X_test2` into **`X_train2_scaled`** and **`X_test2_scaled`** respectively.<br>\n",
    "<I>Hint: fit_transform on the training data and transform only on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_train2_scaled = scaler.fit_transform(X_train2)\n",
    "X_test2_scaled = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Initialize KNN Model named `model_2` with `k=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X_train2_scaled,y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Predict the values for the test data and do an **`accuracy test`** and a **`confusion matrix`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8394160583941606"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model_2.predict(X_test2_scaled)\n",
    "metrics.accuracy_score(y_test2,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  1,  5,  0,  0,  0],\n",
       "       [ 6,  9,  4,  1,  0,  0],\n",
       "       [ 2,  1, 20,  0,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0,  1],\n",
       "       [ 0,  0,  0,  0, 19,  0],\n",
       "       [ 1,  0,  0,  0,  0, 22]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test2,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides accuracy score and confusion matrix, **precision** and **recall** both provide some insights to any classification model that you're trying to train.<br>\n",
    "- **`Precision`** : the percentage of your results which are relevant.\n",
    "$$Precision = \\frac{TP}{TP+FP}$$ \n",
    "where: <br>\n",
    "$TP$ = True positive<br>\n",
    "$FP$ = False positive<br><br>\n",
    "- **`Recall`** :the percentage of total relevant results correctly classified by your algorithm. \n",
    "$$Recall = \\frac{TP}{TP+FN}$$ \n",
    "where: <br>\n",
    "$TP$ = True positive<br>\n",
    "$FN$ = False negative<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76923077 0.45       0.86956522 0.96153846 1.         0.95652174]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.recall_score(y_test2,prediction,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68965517 0.81818182 0.68965517 0.96153846 1.         0.95652174]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.precision_score(y_test2,prediction,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.77      0.73        26\n",
      "           2       0.82      0.45      0.58        20\n",
      "           3       0.69      0.87      0.77        23\n",
      "           5       0.96      0.96      0.96        26\n",
      "           6       1.00      1.00      1.00        19\n",
      "           7       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.84       137\n",
      "   macro avg       0.85      0.83      0.83       137\n",
      "weighted avg       0.85      0.84      0.83       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test2,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally we want to see if the model is overfit by the training data. In such cases we may try to measure the accuracy of the predictions by the training data itself.<br><br>\n",
    "Here we try to compare both the results.<br><br>\n",
    "If the accuracy is not that distinct from that of the test data, the model is well-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9247648902821317\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_train2,model_2.predict(X_train2_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "C.L. Blake and C.J. Merz (1998). UCI repository of machine learning databases. University\n",
    "of California. [www http://www.ics.uci.edu/∼mlearn/MLRepository.html]\n",
    "\n",
    "Kohli, S. (2019, November 18). Understanding a Classification Report For Your Machine Learning Model. Retrieved August 06, 2020, from https://medium.com/@kohlishivam5522/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}